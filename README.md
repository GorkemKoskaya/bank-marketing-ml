# ğŸ¦ Bank Marketing Campaign Target Prediction with Machine Learning (Python)

This project involves building and evaluating machine learning models to predict whether a client will subscribe to a term deposit based on the Bank Marketing Dataset. The focus is on model comparison, class imbalance handling, feature selection, and performance optimization.

## ğŸ“ Dataset

- **Source**: [Kaggle - Bank Marketing Dataset](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets)
- **Records**: 45,211 samples (train-test split applied)
- **Target variable**: `y` (yes = subscribed, no = not subscribed)
- **Features**: 16 attributes related to client and campaign details

## ğŸ§  Models Implemented

We trained and evaluated the following models:

- `model_lg`: Logistic Regression  
- `model_rf`: Random Forest  
- `model_xgb`: XGBoost  
- `model_nb`: Naive Bayes  
- `model_svm`: Support Vector Machine  

Each model was evaluated using:

- Accuracy  
- Precision, Recall, F1-Score  
- ROC-AUC  
- Confusion Matrix  

## âš–ï¸ Handling Class Imbalance

The target class is highly imbalanced (approx. 88% "no", 12% "yes"). To address this, we applied:

- SMOTE (Synthetic Minority Oversampling Technique)  
- Threshold tuning on probability scores  

All experiments were repeated after applying SMOTE and adjusting thresholds for better minority class recall.

## ğŸ“Š Feature Selection

- Feature Importance from XGBoost  
- SHAP (SHapley Additive exPlanations) values  
- Final model trained using selected features based on contribution

## ğŸ§ª Optimization

- Hyperparameter tuning with `GridSearchCV` for XGBoost  
- Optimized thresholds for best F1/Recall trade-off  
- ROC-AUC and classification metrics used to select final model  

## ğŸ“ˆ Visualizations

- SHAP summary plots  
- ROC curves  
- Feature importance bar charts  
- Performance comparison charts across models  

## ğŸ§° Libraries Used

```python
pandas  
numpy  
scikit-learn  
xgboost  
imbalanced-learn  
shap  
matplotlib  
seaborn  
```
### ğŸ“Œ Conclusion

Through this project:

- The classification performance of multiple models was tested.
- The impact of SMOTE and threshold optimization was demonstrated.
- SHAP analysis was used to enhance the explainability of the model's decisions.

ğŸ’¡ **The best results** were achieved using a combination of class balancing, SHAP-based feature selection, and an optimized `XGBoost` model via GridSearch.


# ğŸ‡¹ğŸ‡· BankacÄ±lÄ±k Verisi ile Vadeli Mevduat KampanyasÄ±na KatÄ±lÄ±m Tahmini

Bu proje, **Bank Marketing** veri seti kullanÄ±larak mÃ¼ÅŸterilerin vadeli mevduat teklifine **"evet" veya "hayÄ±r"** deme olasÄ±lÄ±ÄŸÄ±nÄ± sÄ±nÄ±flandÄ±rmayÄ± amaÃ§lamaktadÄ±r. Python dili ile geliÅŸtirilen bu Ã§alÄ±ÅŸma; veri Ã¶n iÅŸleme, model karÅŸÄ±laÅŸtÄ±rmalarÄ±, dengesiz sÄ±nÄ±f yapÄ±sÄ± ile baÅŸ etme yÃ¶ntemleri, hiperparametre optimizasyonu ve model aÃ§Ä±klanabilirliÄŸi gibi adÄ±mlarÄ± kapsamaktadÄ±r.

---

### ğŸ“ Veri Seti
- **Kaynak:** [Kaggle - Bank Marketing Dataset](https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets)
- **Toplam GÃ¶zlem:** 45.211
- **Hedef DeÄŸiÅŸken:** `y` (MÃ¼ÅŸteri abone oldu mu? - yes / no)
- **Ã–zellikler:** YaÅŸ, meslek, eÄŸitim durumu, kredi geÃ§miÅŸi, kampanya iletiÅŸim bilgileri, vb. 16 deÄŸiÅŸken.

---

### ğŸ§  Uygulanan Modeller

Temel sÄ±nÄ±flandÄ±rma algoritmalarÄ± uygulanmÄ±ÅŸtÄ±r:

- `model_lg` â€“ Lojistik Regresyon  
- `model_rf` â€“ Rastgele Orman  
- `model_xgb` â€“ XGBoost  
- `model_svm` â€“ Destek VektÃ¶r Makineleri  
- `model_nb` â€“ Naive Bayes  

Her model, aÅŸaÄŸÄ±daki metriklerle deÄŸerlendirilmiÅŸtir:

- DoÄŸruluk (Accuracy)  
- ROC AUC  
- F1-Score  
- Precision / Recall

---

### âš–ï¸ SÄ±nÄ±f DengesizliÄŸi

Veri setindeki dengesiz sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± (%88 HayÄ±r, %12 Evet) nedeniyle aÅŸaÄŸÄ±daki yÃ¶ntemler uygulanmÄ±ÅŸtÄ±r:

- `SMOTE` (Synthetic Minority Over-sampling)
- AÄŸÄ±rlÄ±klÄ± sÄ±nÄ±f yaklaÅŸÄ±mlarÄ±
- EÅŸik (Threshold) optimizasyonu
- SHAP deÄŸerlerine gÃ¶re Ã¶nemli deÄŸiÅŸken seÃ§imi

---

### ğŸ”§ Hiperparametre AyarlamasÄ±

`GridSearchCV` ile optimize edilmiÅŸ parametreler:

```python
param_grid = {
    "learning_rate": [0.01, 0.1],
    "max_depth": [3, 5, 7],
    "n_estimators": [100, 200],
    "subsample": [0.8, 1],
    "colsample_bytree": [0.8, 1]
}
```

---

### ğŸ“Š GÃ¶rselleÅŸtirmeler

- SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± grafiÄŸi  
- ROC eÄŸrileri  
- Precision / Recall karÅŸÄ±laÅŸtÄ±rmalarÄ±  
- Feature importance (XGBoost & SHAP)  
- SHAP summary & bar plot  
- SMOTE sonrasÄ± model performanslarÄ±  
- Threshold optimizasyonu sonrasÄ± deÄŸerlendirme

---

### ğŸ§® KullanÄ±lan KÃ¼tÃ¼phaneler

```python
pandas, numpy, matplotlib, seaborn  
sklearn, xgboost, imblearn  
shap, joblib
```

---

### ğŸ“Œ SonuÃ§

Bu proje sayesinde:

- FarklÄ± modellerin sÄ±nÄ±flandÄ±rma performansÄ± test edilmiÅŸtir.  
- SMOTE ve eÅŸik optimizasyonunun etkisi gÃ¶sterilmiÅŸtir.  
- SHAP analizi ile modelin kararlarÄ±na aÃ§Ä±klÄ±k getirilmiÅŸtir.  

ğŸ’¡ **En iyi sonuÃ§**, sÄ±nÄ±f dengesizliÄŸi + SHAP temelli deÄŸiÅŸken seÃ§imi + `XGBoost` modelinin grid search sonrasÄ± optimize edilmesi ile elde edilmiÅŸtir.
